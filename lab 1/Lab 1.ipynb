{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91089ffe-c0a6-4b97-b966-9a2ac8569bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>department</th><th>name</th></tr></thead><tbody><tr><td>28</td><td>IT</td><td>Jan</td></tr><tr><td>34</td><td>HR</td><td>Anna</td></tr><tr><td>22</td><td>Finance</td><td>Piotr</td></tr><tr><td>45</td><td>IT</td><td>Ewa</td></tr><tr><td>31</td><td>HR</td><td>Marek</td></tr><tr><td>29</td><td>Finance</td><td>Joanna</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         28,
         "IT",
         "Jan"
        ],
        [
         34,
         "HR",
         "Anna"
        ],
        [
         22,
         "Finance",
         "Piotr"
        ],
        [
         45,
         "IT",
         "Ewa"
        ],
        [
         31,
         "HR",
         "Marek"
        ],
        [
         29,
         "Finance",
         "Joanna"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.)\n",
    "pracownicy_data = [\n",
    "    {'name': 'Jan', 'age': 28, 'department': 'IT'},\n",
    "    {'name': 'Anna', 'age': 34, 'department': 'HR'},\n",
    "    {'name': 'Piotr', 'age': 22, 'department': 'Finance'},\n",
    "    {'name': 'Ewa', 'age': 45, 'department': 'IT'},\n",
    "    {'name': 'Marek', 'age': 31, 'department': 'HR'},\n",
    "    {'name': 'Joanna', 'age': 29, 'department': 'Finance'}\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(pracownicy_data)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae6e706-b25c-473b-b35c-7bbf5aa78161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|database|volume_name|\n+--------+-----------+\n| default| pracownicy|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW VOLUMES IN big_data.default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fcd81a-cc6b-46ef-a7ec-575c54d3437c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>��n\u0000a\u0000m\u0000e\u0000</th><th>\u0000a\u0000g\u0000e\u0000</th><th>\u0000d\u0000e\u0000p\u0000a\u0000r\u0000t\u0000m\u0000e\u0000n\u0000t\u0000</th></tr></thead><tbody><tr><td>\u0000J\u0000a\u0000n\u0000</td><td>\u00002\u00008\u0000</td><td>\u0000I\u0000T\u0000</td></tr><tr><td>\u0000A\u0000n\u0000n\u0000a\u0000</td><td>\u00003\u00004\u0000</td><td>\u0000H\u0000R\u0000</td></tr><tr><td>\u0000P\u0000i\u0000o\u0000t\u0000r\u0000</td><td>\u00002\u00002\u0000</td><td>\u0000F\u0000i\u0000n\u0000a\u0000n\u0000c\u0000e\u0000</td></tr><tr><td>\u0000E\u0000w\u0000a\u0000</td><td>\u00004\u00005\u0000</td><td>\u0000I\u0000T\u0000</td></tr><tr><td>\u0000M\u0000a\u0000r\u0000e\u0000k\u0000</td><td>\u00003\u00001\u0000</td><td>\u0000H\u0000R\u0000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "\u0000J\u0000a\u0000n\u0000",
         "\u00002\u00008\u0000",
         "\u0000I\u0000T\u0000"
        ],
        [
         "\u0000A\u0000n\u0000n\u0000a\u0000",
         "\u00003\u00004\u0000",
         "\u0000H\u0000R\u0000"
        ],
        [
         "\u0000P\u0000i\u0000o\u0000t\u0000r\u0000",
         "\u00002\u00002\u0000",
         "\u0000F\u0000i\u0000n\u0000a\u0000n\u0000c\u0000e\u0000"
        ],
        [
         "\u0000E\u0000w\u0000a\u0000",
         "\u00004\u00005\u0000",
         "\u0000I\u0000T\u0000"
        ],
        [
         "\u0000M\u0000a\u0000r\u0000e\u0000k\u0000",
         "\u00003\u00001\u0000",
         "\u0000H\u0000R\u0000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "��n\u0000a\u0000m\u0000e\u0000",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "\u0000a\u0000g\u0000e\u0000",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "\u0000d\u0000e\u0000p\u0000a\u0000r\u0000t\u0000m\u0000e\u0000n\u0000t\u0000",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.)\n",
    "file_path = \"/Volumes/big_data/default/pracownicy/pracownicy.csv\"\n",
    "\n",
    "# Load the data again\n",
    "df_employees = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "display(df_employees.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3194f9cc-aedb-486c-9699-5f889526c827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['name', 'age', 'department']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Anna</td><td>34</td></tr><tr><td>Ewa</td><td>45</td></tr><tr><td>Marek</td><td>31</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Anna",
         34
        ],
        [
         "Ewa",
         45
        ],
        [
         "Marek",
         31
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. )\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "file_path = \"/Volumes/big_data/default/pracownicy/pracownicy.csv\"\n",
    "\n",
    "df_employees = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-16\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# 2. Verify columns are clean now (Optional)\n",
    "print(\"Columns:\", df_employees.columns)\n",
    "\n",
    "# 3. Filter and Select\n",
    "df_filtered = df_employees.filter(col(\"age\") > 30) \\\n",
    "                          .select(\"name\", \"age\")\n",
    "\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b50def-e550-481b-ab2a-1b6b09ba2382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by age (descending):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th></tr></thead><tbody><tr><td>Ewa</td><td>45</td><td>IT</td></tr><tr><td>Anna</td><td>34</td><td>HR</td></tr><tr><td>Marek</td><td>31</td><td>HR</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td></tr><tr><td>Jan</td><td>28</td><td>IT</td></tr><tr><td>Piotr</td><td>22</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Ewa",
         45,
         "IT"
        ],
        [
         "Anna",
         34,
         "HR"
        ],
        [
         "Marek",
         31,
         "HR"
        ],
        [
         "Joanna",
         29,
         "Finance"
        ],
        [
         "Jan",
         28,
         "IT"
        ],
        [
         "Piotr",
         22,
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age by department:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>average_age</th></tr></thead><tbody><tr><td>HR</td><td>32.5</td></tr><tr><td>IT</td><td>36.5</td></tr><tr><td>Finance</td><td>25.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         32.5
        ],
        [
         "IT",
         36.5
        ],
        [
         "Finance",
         25.5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "average_age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. )\n",
    "\n",
    "from pyspark.sql.functions import col, avg, desc\n",
    "\n",
    "df_sorted = df_employees.orderBy(col(\"age\").desc())\n",
    "\n",
    "print(\"Sorted by age (descending):\")\n",
    "display(df_sorted)\n",
    "\n",
    "df_grouped = df_employees.groupBy(\"department\") \\\n",
    "                         .agg(avg(\"age\").alias(\"average_age\"))\n",
    "\n",
    "print(\"Average age by department:\")\n",
    "display(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcc9840-3a40-4c8a-b03d-44efb571611a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>age_in_5_years</th></tr></thead><tbody><tr><td>Jan</td><td>28</td><td>33</td></tr><tr><td>Anna</td><td>34</td><td>39</td></tr><tr><td>Piotr</td><td>22</td><td>27</td></tr><tr><td>Ewa</td><td>45</td><td>50</td></tr><tr><td>Marek</td><td>31</td><td>36</td></tr><tr><td>Joanna</td><td>29</td><td>34</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan",
         28,
         33
        ],
        [
         "Anna",
         34,
         39
        ],
        [
         "Piotr",
         22,
         27
        ],
        [
         "Ewa",
         45,
         50
        ],
        [
         "Marek",
         31,
         36
        ],
        [
         "Joanna",
         29,
         34
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "age_in_5_years",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 5. )\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_modified = df_employees.withColumn(\"age_in_5_years\", col(\"age\") + 5)\n",
    "\n",
    "df_final = df_modified.drop(\"department\")\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dba6c92-3ac9-4fa6-90d8-af94bd716302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects Data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>projects</th></tr></thead><tbody><tr><td>Jan</td><td>5</td></tr><tr><td>Anna</td><td>3</td></tr><tr><td>Piotr</td><td>4</td></tr><tr><td>Ewa</td><td>7</td></tr><tr><td>Marek</td><td>2</td></tr><tr><td>Joanna</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan",
         5
        ],
        [
         "Anna",
         3
        ],
        [
         "Piotr",
         4
        ],
        [
         "Ewa",
         7
        ],
        [
         "Marek",
         2
        ],
        [
         "Joanna",
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "projects",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Data (Employees + Projects):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th><th>projects</th></tr></thead><tbody><tr><td>Jan</td><td>28</td><td>IT</td><td>5</td></tr><tr><td>Anna</td><td>34</td><td>HR</td><td>3</td></tr><tr><td>Piotr</td><td>22</td><td>Finance</td><td>4</td></tr><tr><td>Ewa</td><td>45</td><td>IT</td><td>7</td></tr><tr><td>Marek</td><td>31</td><td>HR</td><td>2</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan",
         28,
         "IT",
         5
        ],
        [
         "Anna",
         34,
         "HR",
         3
        ],
        [
         "Piotr",
         22,
         "Finance",
         4
        ],
        [
         "Ewa",
         45,
         "IT",
         7
        ],
        [
         "Marek",
         31,
         "HR",
         2
        ],
        [
         "Joanna",
         29,
         "Finance",
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "projects",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. )\n",
    "\n",
    "projects_file_path = \"/Volumes/big_data/default/pracownicy/projekty.csv\"\n",
    "\n",
    "df_projects = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .load(projects_file_path)\n",
    "\n",
    "print(\"Projects Data:\")\n",
    "display(df_projects)\n",
    "\n",
    "\n",
    "df_joined = df_employees.join(df_projects, on=\"name\", how=\"inner\")\n",
    "\n",
    "print(\"Joined Data (Employees + Projects):\")\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f36da1-4eaf-4190-8f4d-8d0d83254b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with NULLs:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th></tr></thead><tbody><tr><td>Jan</td><td>null</td><td>IT</td></tr><tr><td>Anna</td><td>34</td><td>null</td></tr><tr><td>Piotr</td><td>22</td><td>Finance</td></tr><tr><td>Ewa</td><td>45</td><td>IT</td></tr><tr><td>Marek</td><td>31</td><td>HR</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan",
         null,
         "IT"
        ],
        [
         "Anna",
         34,
         null
        ],
        [
         "Piotr",
         22,
         "Finance"
        ],
        [
         "Ewa",
         45,
         "IT"
        ],
        [
         "Marek",
         31,
         "HR"
        ],
        [
         "Joanna",
         29,
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after filling NULLs:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th></tr></thead><tbody><tr><td>Jan</td><td>0</td><td>IT</td></tr><tr><td>Anna</td><td>34</td><td>Unknown</td></tr><tr><td>Piotr</td><td>22</td><td>Finance</td></tr><tr><td>Ewa</td><td>45</td><td>IT</td></tr><tr><td>Marek</td><td>31</td><td>HR</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Jan",
         0,
         "IT"
        ],
        [
         "Anna",
         34,
         "Unknown"
        ],
        [
         "Piotr",
         22,
         "Finance"
        ],
        [
         "Ewa",
         45,
         "IT"
        ],
        [
         "Marek",
         31,
         "HR"
        ],
        [
         "Joanna",
         29,
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping rows with NULLs:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th></tr></thead><tbody><tr><td>Piotr</td><td>22</td><td>Finance</td></tr><tr><td>Ewa</td><td>45</td><td>IT</td></tr><tr><td>Marek</td><td>31</td><td>HR</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Piotr",
         22,
         "Finance"
        ],
        [
         "Ewa",
         45,
         "IT"
        ],
        [
         "Marek",
         31,
         "HR"
        ],
        [
         "Joanna",
         29,
         "Finance"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. )\n",
    "\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "df_with_nulls = df_employees.withColumn(\"age\", \n",
    "                                        when(col(\"name\") == \"Jan\", None)\n",
    "                                        .otherwise(col(\"age\"))) \\\n",
    "                            .withColumn(\"department\", \n",
    "                                        when(col(\"name\") == \"Anna\", None)\n",
    "                                        .otherwise(col(\"department\")))\n",
    "\n",
    "print(\"DataFrame with NULLs:\")\n",
    "display(df_with_nulls)\n",
    "\n",
    "\n",
    "df_filled = df_with_nulls.na.fill({\n",
    "    \"age\": 0, \n",
    "    \"department\": \"Unknown\"\n",
    "})\n",
    "\n",
    "print(\"DataFrame after filling NULLs:\")\n",
    "display(df_filled)\n",
    "\n",
    "\n",
    "df_dropped = df_with_nulls.na.drop(how=\"any\")\n",
    "\n",
    "print(\"DataFrame after dropping rows with NULLs:\")\n",
    "display(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e94752-c6a3-447a-a94f-914469f1858d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees ranked by project count:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>department</th><th>projects</th><th>rank</th></tr></thead><tbody><tr><td>Ewa</td><td>45</td><td>IT</td><td>7</td><td>1</td></tr><tr><td>Joanna</td><td>29</td><td>Finance</td><td>6</td><td>2</td></tr><tr><td>Jan</td><td>28</td><td>IT</td><td>5</td><td>3</td></tr><tr><td>Piotr</td><td>22</td><td>Finance</td><td>4</td><td>4</td></tr><tr><td>Anna</td><td>34</td><td>HR</td><td>3</td><td>5</td></tr><tr><td>Marek</td><td>31</td><td>HR</td><td>2</td><td>6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Ewa",
         45,
         "IT",
         7,
         1
        ],
        [
         "Joanna",
         29,
         "Finance",
         6,
         2
        ],
        [
         "Jan",
         28,
         "IT",
         5,
         3
        ],
        [
         "Piotr",
         22,
         "Finance",
         4,
         4
        ],
        [
         "Anna",
         34,
         "HR",
         3,
         5
        ],
        [
         "Marek",
         31,
         "HR",
         2,
         6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "projects",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8.)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, desc\n",
    "\n",
    "window_spec = Window.orderBy(col(\"projects\").desc())\n",
    "\n",
    "df_ranked = df_joined.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "print(\"Employees ranked by project count:\")\n",
    "display(df_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f80dae9-67e5-4dd0-a684-1955016b66f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 162 bytes.\nRaw text DataFrame:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>Apache Spark is fast.</td></tr><tr><td>Spark is a unified analytics engine for Big Data.</td></tr><tr><td>Big Data requires distributed computing.</td></tr><tr><td>Spark uses DataFrames for distributed processing.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Apache Spark is fast."
        ],
        [
         "Spark is a unified analytics engine for Big Data."
        ],
        [
         "Big Data requires distributed computing."
        ],
        [
         "Spark uses DataFrames for distributed processing."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most frequent words:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>word</th><th>count</th></tr></thead><tbody><tr><td>spark</td><td>3</td></tr><tr><td>distributed</td><td>2</td></tr><tr><td>big</td><td>2</td></tr><tr><td>is</td><td>2</td></tr><tr><td>for</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark",
         3
        ],
        [
         "distributed",
         2
        ],
        [
         "big",
         2
        ],
        [
         "is",
         2
        ],
        [
         "for",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "word",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9.)\n",
    "\n",
    "from pyspark.sql.functions import col, split, explode, lower, trim, count\n",
    "\n",
    "\n",
    "file_path = \"/Volumes/big_data/default/pracownicy/word_count_sample.txt\"\n",
    "dbutils.fs.put(file_path, sample_text, True)\n",
    "\n",
    "df_text = spark.read.text(file_path)\n",
    "\n",
    "print(\"Raw text DataFrame:\")\n",
    "display(df_text)\n",
    "\n",
    "df_words = df_text.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "\n",
    "df_counts = df_words.select(lower(trim(col(\"word\"))).alias(\"word\")) \\\n",
    "                    .filter(col(\"word\") != \"\") \\\n",
    "                    .groupBy(\"word\") \\\n",
    "                    .count() \\\n",
    "                    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"Top 5 most frequent words:\")\n",
    "display(df_counts.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc99b369-ea7e-4002-a1e1-0f42a55a9fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to save to Volume: /Volumes/main/default/my_files/moje_wyniki\nError: Make sure the Volume 'my_files' exists in 'main.default'.\n[NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.analysis.NoSuchCatalogException\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl$ConvertVolumeException.convertVolumeException(ManagedCatalogClientImpl.scala:9145)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl$ConvertVolumeException.convertVolumeException(ManagedCatalogClientImpl.scala:9151)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl$ConvertVolumeException.convertVolumeException(ManagedCatalogClientImpl.scala:9165)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.handleVolumeException(ManagedCatalogClientImpl.scala:9133)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$getVolume$1(ManagedCatalogClientImpl.scala:8867)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7934)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7933)\n\tat com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException(ErrorDetailsHandler.scala:96)\n\tat com.databricks.sql.managedcatalog.client.ErrorDetailsHandlerImpl.wrapServiceException$(ErrorDetailsHandler.scala:88)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:44)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7912)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7896)\n\tat com.databricks.sql.managedcatalog.client.ManagedCatalogClientImpl.getVolume(ManagedCatalogClientImpl.scala:8862)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.getVolume(ManagedCatalogCommon.scala:3939)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$getVolume$1(ProfiledManagedCatalog.scala:1486)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:2065)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:74)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:73)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.getVolume(ProfiledManagedCatalog.scala:1486)\n\tat com.databricks.sql.acl.fs.volumes.VolumePath$.$anonfun$registerSAM$1(VolumePath.scala:271)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.acl.fs.volumes.VolumePath$.registerSAM(VolumePath.scala:259)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:283)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerPathAccess(CredentialScopeSQLHelper.scala:1171)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.injectUnityCatalogCredential(ResolveWithCredential.scala:301)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.$anonfun$injectUnityCatalogCredential$7(ResolveWithCredential.scala:340)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.$anonfun$injectUnityCatalogCredential$7$adapted(ResolveWithCredential.scala:333)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.$anonfun$injectUnityCatalogCredential$6(ResolveWithCredential.scala:333)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.$anonfun$injectUnityCatalogCredential$6$adapted(ResolveWithCredential.scala:332)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$.injectUnityCatalogCredential(ResolveWithCredential.scala:332)\n\tat org.apache.spark.sql.classic.DataFrameWriter.getOptionsWithPath(DataFrameWriter.scala:319)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:163)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:152)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4042)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3425)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\n"
     ]
    }
   ],
   "source": [
    "catalog_name = \"main\"      \n",
    "schema_name = \"default\"   \n",
    "volume_name = \"my_files\"  \n",
    "\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/moje_wyniki\"\n",
    "\n",
    "print(f\"Attempting to save to Volume: {volume_path}\")\n",
    "\n",
    "try:\n",
    "    # Save to CSV\n",
    "    df_joined.write.mode(\"overwrite\").csv(f\"{volume_path}/csv\", header=True)\n",
    "    \n",
    "    # Save to JSON\n",
    "    df_joined.write.mode(\"overwrite\").json(f\"{volume_path}/json\")\n",
    "    \n",
    "    print(\"Success! Files saved to Volume.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Make sure the Volume '{volume_name}' exists in '{catalog_name}.{schema_name}'.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fac212e-1310-4c06-b0e5-5d54c41796ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}